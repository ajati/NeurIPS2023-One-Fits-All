train 74382
val 108675
test 219324
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Model params that has gradient
gpt2.wpe.weight: requires_grad=True : #params=786432
gpt2.h.0.ln_1.weight: requires_grad=True : #params=768
gpt2.h.0.ln_1.bias: requires_grad=True : #params=768
gpt2.h.0.ln_2.weight: requires_grad=True : #params=768
gpt2.h.0.ln_2.bias: requires_grad=True : #params=768
gpt2.h.1.ln_1.weight: requires_grad=True : #params=768
gpt2.h.1.ln_1.bias: requires_grad=True : #params=768
gpt2.h.1.ln_2.weight: requires_grad=True : #params=768
gpt2.h.1.ln_2.bias: requires_grad=True : #params=768
gpt2.h.2.ln_1.weight: requires_grad=True : #params=768
gpt2.h.2.ln_1.bias: requires_grad=True : #params=768
gpt2.h.2.ln_2.weight: requires_grad=True : #params=768
gpt2.h.2.ln_2.bias: requires_grad=True : #params=768
gpt2.h.3.ln_1.weight: requires_grad=True : #params=768
gpt2.h.3.ln_1.bias: requires_grad=True : #params=768
gpt2.h.3.ln_2.weight: requires_grad=True : #params=768
gpt2.h.3.ln_2.bias: requires_grad=True : #params=768
gpt2.h.4.ln_1.weight: requires_grad=True : #params=768
gpt2.h.4.ln_1.bias: requires_grad=True : #params=768
gpt2.h.4.ln_2.weight: requires_grad=True : #params=768
gpt2.h.4.ln_2.bias: requires_grad=True : #params=768
gpt2.h.5.ln_1.weight: requires_grad=True : #params=768
gpt2.h.5.ln_1.bias: requires_grad=True : #params=768
gpt2.h.5.ln_2.weight: requires_grad=True : #params=768
gpt2.h.5.ln_2.bias: requires_grad=True : #params=768
gpt2.ln_f.weight: requires_grad=True : #params=768
gpt2.ln_f.bias: requires_grad=True : #params=768
in_layer.weight: requires_grad=True : #params=12288
in_layer.bias: requires_grad=True : #params=768
out_layer.weight: requires_grad=True : #params=2433024
out_layer.bias: requires_grad=True : #params=96
Total #params manual = 3252576
VJ: total learnable params:  3252576
VJ: total params:  84358752
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
-------------------------------------------------- MACs across 10% train data = 304.316G nparams = 2.466M
self vs thop trainable params 3252576 2.466M
0it [00:00, ?it/s]1it [00:00,  2.14it/s]2it [00:01,  1.57it/s]3it [00:01,  1.52it/s]4it [00:02,  1.49it/s]5it [00:03,  1.48it/s]6it [00:04,  1.40it/s]7it [00:04,  1.38it/s]8it [00:05,  1.38it/s]9it [00:06,  1.37it/s]10it [00:07,  1.35it/s]11it [00:07,  1.37it/s]12it [00:08,  1.38it/s]13it [00:09,  1.37it/s]14it [00:09,  1.37it/s]15it [00:10,  1.36it/s]16it [00:11,  1.39it/s]17it [00:12,  1.41it/s]18it [00:12,  1.39it/s]19it [00:13,  1.39it/s]20it [00:14,  1.38it/s]21it [00:14,  1.40it/s]22it [00:15,  1.39it/s]23it [00:16,  1.40it/s]24it [00:17,  1.39it/s]25it [00:17,  1.40it/s]26it [00:18,  1.38it/s]27it [00:19,  1.36it/s]28it [00:20,  1.39it/s]29it [00:20,  1.41it/s]30it [00:21,  1.43it/s]31it [00:22,  1.44it/s]32it [00:22,  1.44it/s]33it [00:23,  1.45it/s]34it [00:24,  1.45it/s]35it [00:24,  1.45it/s]36it [00:25,  1.45it/s]37it [00:26,  1.45it/s]38it [00:26,  1.46it/s]39it [00:27,  1.46it/s]40it [00:28,  1.46it/s]41it [00:28,  1.46it/s]42it [00:29,  1.46it/s]43it [00:30,  1.46it/s]44it [00:30,  1.46it/s]45it [00:31,  1.46it/s]46it [00:32,  1.46it/s]47it [00:33,  1.46it/s]48it [00:33,  1.46it/s]49it [00:34,  1.46it/s]50it [00:35,  1.46it/s]51it [00:35,  1.46it/s]52it [00:36,  1.46it/s]52it [00:36,  1.42it/s]
-------------------------------------------------- Epoch: 1 cost time: 37.22523832321167
0it [00:00, ?it/s]1it [00:00,  1.54it/s]2it [00:01,  2.06it/s]3it [00:01,  2.30it/s]4it [00:01,  2.44it/s]5it [00:02,  2.50it/s]6it [00:02,  2.56it/s]7it [00:02,  2.60it/s]8it [00:03,  2.64it/s]9it [00:03,  2.65it/s]10it [00:04,  2.67it/s]11it [00:04,  2.53it/s]12it [00:04,  2.55it/s]13it [00:05,  2.53it/s]14it [00:05,  2.57it/s]15it [00:05,  2.60it/s]16it [00:06,  2.63it/s]17it [00:06,  2.64it/s]18it [00:07,  2.66it/s]19it [00:07,  2.66it/s]20it [00:07,  2.67it/s]21it [00:08,  2.66it/s]22it [00:08,  2.67it/s]23it [00:08,  2.64it/s]24it [00:09,  2.66it/s]25it [00:09,  2.64it/s]26it [00:10,  2.66it/s]27it [00:10,  2.65it/s]28it [00:10,  2.67it/s]29it [00:11,  2.67it/s]30it [00:11,  2.68it/s]31it [00:11,  2.67it/s]32it [00:12,  2.57it/s]33it [00:12,  2.48it/s]34it [00:13,  2.45it/s]35it [00:13,  2.50it/s]36it [00:14,  2.55it/s]37it [00:14,  2.56it/s]38it [00:14,  2.61it/s]39it [00:15,  2.61it/s]40it [00:15,  2.60it/s]41it [00:15,  2.50it/s]42it [00:16,  2.50it/s]43it [00:16,  2.54it/s]44it [00:17,  2.58it/s]45it [00:17,  2.60it/s]46it [00:17,  2.63it/s]47it [00:18,  2.63it/s]48it [00:18,  2.63it/s]49it [00:19,  2.64it/s]50it [00:19,  2.66it/s]51it [00:19,  2.64it/s]52it [00:20,  2.66it/s]53it [00:20,  2.63it/s]54it [00:20,  2.66it/s]55it [00:21,  2.65it/s]56it [00:21,  2.65it/s]57it [00:22,  2.64it/s]58it [00:22,  2.62it/s]59it [00:22,  2.63it/s]60it [00:23,  2.65it/s]61it [00:23,  2.64it/s]62it [00:23,  2.66it/s]63it [00:24,  2.66it/s]64it [00:24,  2.67it/s]65it [00:25,  2.66it/s]66it [00:25,  2.67it/s]67it [00:25,  2.65it/s]68it [00:26,  2.67it/s]69it [00:26,  2.66it/s]70it [00:26,  2.68it/s]71it [00:27,  2.65it/s]72it [00:27,  2.66it/s]73it [00:28,  2.65it/s]74it [00:28,  2.66it/s]75it [00:28,  2.66it/s]76it [00:29,  2.68it/s]77it [00:29,  2.67it/s]77it [00:29,  2.60it/s]
Epoch: 1, Steps: 52 | Train Loss: 0.2211129 Vali Loss: 0.4665574
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
Validation loss decreased (inf --> 0.466557).  Saving model ...
------------------------------------
0it [00:00, ?it/s]1it [00:00,  2.35it/s]2it [00:00,  2.75it/s]3it [00:01,  2.86it/s]4it [00:01,  2.94it/s]5it [00:01,  2.99it/s]6it [00:02,  3.04it/s]7it [00:02,  2.99it/s]8it [00:02,  2.95it/s]9it [00:03,  2.99it/s]10it [00:03,  2.96it/s]11it [00:03,  2.90it/s]12it [00:04,  2.94it/s]13it [00:04,  2.97it/s]14it [00:04,  2.97it/s]15it [00:05,  2.94it/s]16it [00:05,  2.99it/s]17it [00:05,  2.99it/s]18it [00:06,  3.03it/s]19it [00:06,  3.03it/s]20it [00:06,  3.02it/s]21it [00:07,  3.02it/s]22it [00:07,  3.04it/s]23it [00:07,  3.04it/s]24it [00:08,  3.07it/s]25it [00:08,  3.07it/s]26it [00:08,  3.10it/s]27it [00:09,  3.06it/s]28it [00:09,  3.09it/s]29it [00:09,  3.08it/s]30it [00:09,  3.10it/s]31it [00:10,  3.10it/s]32it [00:10,  3.10it/s]33it [00:10,  3.08it/s]34it [00:11,  3.09it/s]35it [00:11,  3.07it/s]36it [00:11,  3.09it/s]37it [00:12,  3.09it/s]38it [00:12,  3.09it/s]39it [00:12,  3.07it/s]40it [00:13,  3.05it/s]41it [00:13,  3.00it/s]42it [00:13,  3.00it/s]43it [00:14,  2.98it/s]44it [00:14,  3.03it/s]45it [00:14,  3.00it/s]46it [00:15,  3.05it/s]47it [00:15,  3.01it/s]48it [00:15,  3.01it/s]49it [00:16,  3.02it/s]50it [00:16,  2.98it/s]51it [00:16,  3.01it/s]52it [00:17,  3.02it/s]53it [00:17,  2.99it/s]54it [00:17,  3.04it/s]55it [00:18,  3.00it/s]56it [00:18,  3.01it/s]57it [00:18,  3.03it/s]58it [00:19,  3.07it/s]59it [00:19,  3.02it/s]60it [00:19,  2.99it/s]61it [00:20,  2.99it/s]62it [00:20,  2.94it/s]63it [00:20,  2.97it/s]64it [00:21,  3.02it/s]65it [00:21,  3.04it/s]66it [00:21,  3.07it/s]67it [00:22,  3.03it/s]68it [00:22,  3.05it/s]69it [00:22,  3.01it/s]70it [00:23,  2.95it/s]71it [00:23,  2.89it/s]72it [00:23,  2.87it/s]73it [00:24,  2.92it/s]74it [00:24,  2.97it/s]75it [00:24,  3.00it/s]76it [00:25,  3.05it/s]77it [00:25,  3.00it/s]78it [00:25,  2.95it/s]79it [00:26,  2.96it/s]80it [00:26,  3.02it/s]81it [00:26,  3.03it/s]82it [00:27,  3.07it/s]83it [00:27,  3.07it/s]84it [00:27,  3.10it/s]85it [00:28,  3.08it/s]86it [00:28,  3.08it/s]87it [00:28,  3.08it/s]88it [00:29,  3.10it/s]89it [00:29,  3.06it/s]90it [00:29,  3.01it/s]91it [00:30,  2.98it/s]92it [00:30,  2.93it/s]93it [00:30,  2.98it/s]94it [00:31,  3.01it/s]95it [00:31,  2.97it/s]96it [00:31,  2.90it/s]97it [00:32,  2.94it/s]98it [00:32,  2.90it/s]99it [00:32,  2.91it/s]100it [00:33,  2.94it/s]101it [00:33,  2.89it/s]102it [00:33,  2.97it/s]103it [00:34,  3.00it/s]104it [00:34,  3.05it/s]105it [00:34,  3.06it/s]106it [00:35,  3.09it/s]107it [00:35,  3.07it/s]108it [00:35,  3.10it/s]109it [00:36,  3.08it/s]110it [00:36,  3.00it/s]111it [00:36,  3.03it/s]112it [00:37,  3.01it/s]113it [00:37,  2.95it/s]114it [00:37,  2.91it/s]115it [00:38,  2.96it/s]116it [00:38,  3.00it/s]117it [00:38,  3.00it/s]118it [00:39,  2.96it/s]119it [00:39,  2.95it/s]120it [00:39,  2.97it/s]121it [00:40,  2.99it/s]122it [00:40,  2.99it/s]123it [00:40,  2.97it/s]124it [00:41,  3.00it/s]125it [00:41,  3.01it/s]126it [00:41,  3.05it/s]127it [00:42,  2.96it/s]128it [00:42,  3.02it/s]129it [00:42,  2.99it/s]130it [00:43,  3.04it/s]131it [00:43,  3.05it/s]132it [00:43,  3.08it/s]133it [00:44,  3.00it/s]134it [00:44,  2.96it/s]135it [00:44,  3.00it/s]136it [00:45,  3.01it/s]137it [00:45,  3.03it/s]138it [00:45,  3.07it/s]139it [00:46,  3.04it/s]140it [00:46,  3.08it/s]141it [00:46,  3.06it/s]142it [00:47,  3.07it/s]143it [00:47,  3.06it/s]144it [00:47,  3.09it/s]145it [00:48,  3.10it/s]146it [00:48,  3.08it/s]147it [00:48,  3.03it/s]148it [00:49,  3.04it/s]149it [00:49,  3.05it/s]150it [00:49,  3.09it/s]151it [00:50,  3.09it/s]152it [00:50,  3.09it/s]153it [00:50,  3.07it/s]154it [00:51,  3.10it/s]155it [00:51,  3.10it/s]155it [00:51,  3.01it/s]
test shape: (155, 1408, 96, 1) (155, 1408, 96, 1)
test shape: (218240, 96, 1) (218240, 96, 1)
mae:0.2225, mse:0.1694, rmse:0.4116, smape:57.3041
-------------------------------------------------- TEST time = 52.53940510749817
mse_mean = 0.1694, mse_std = 0.0000
mae_mean = 0.2225, mae_std = 0.0000
****************************************************************************************************
======================================== Runtime metrics ========================================
MACs = 304.316G
params = 2.466M
total params = 84.359M
epoch time = 37.22523832321167
max memory = 21.91 GB
test time = 52.53940510749817
****************************************************************************************************
